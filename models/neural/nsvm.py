"""
Neural Stochastic Volatility Model (NSVM) â€” uses paired recurrent neural
networks to model non-linear, potentially non-Markovian volatility dynamics.

Architecture (from Luo et al., "Neural Stochastic Volatility Model"):
    - **Generative network**: RNN that models the joint dynamics of returns
      and latent volatility.
    - **Inference network**: RNN that approximates the posterior over latent
      volatility given observed returns (amortised variational inference).

The model is trained with ELBO (Evidence Lower Bound) maximization:
    ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z))

After training, Monte Carlo paths are generated by:
    1. Running the inference network on recent data to get a volatility state.
    2. Unrolling the generative network forward with sampled latent states.
"""

from __future__ import annotations

import logging
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from models.base import BaseForecaster

logger = logging.getLogger(__name__)


class GenerativeRNN(nn.Module):
    """Generative model: p(x_t, z_t | z_{t-1})."""

    def __init__(self, z_dim: int = 16, hidden_dim: int = 64) -> None:
        super().__init__()
        self.z_dim = z_dim
        self.rnn = nn.GRUCell(input_size=z_dim + 1, hidden_size=hidden_dim)
        # Prior on z_t given h_{t-1}
        self.prior_mu = nn.Linear(hidden_dim, z_dim)
        self.prior_logvar = nn.Linear(hidden_dim, z_dim)
        # Emission: p(x_t | z_t, h_t)
        self.emission_mu = nn.Linear(hidden_dim + z_dim, 1)
        self.emission_logvar = nn.Linear(hidden_dim + z_dim, 1)

    def forward(
        self,
        x_prev: torch.Tensor,
        z_prev: torch.Tensor,
        h_prev: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Single step of the generative model.

        Returns (h_t, prior_mu, prior_logvar, emit_mu, emit_logvar)
        """
        inp = torch.cat([x_prev, z_prev], dim=-1)
        h_t = self.rnn(inp, h_prev)
        p_mu = self.prior_mu(h_t)
        p_logvar = self.prior_logvar(h_t)
        hz = torch.cat([h_t, z_prev], dim=-1)
        e_mu = self.emission_mu(hz)
        e_logvar = self.emission_logvar(hz)
        return h_t, p_mu, p_logvar, e_mu, e_logvar


class InferenceRNN(nn.Module):
    """Inference model: q(z_t | x_{1:t})."""

    def __init__(self, z_dim: int = 16, hidden_dim: int = 64) -> None:
        super().__init__()
        self.rnn = nn.GRUCell(input_size=1, hidden_size=hidden_dim)
        self.q_mu = nn.Linear(hidden_dim, z_dim)
        self.q_logvar = nn.Linear(hidden_dim, z_dim)

    def forward(
        self,
        x_t: torch.Tensor,
        h_prev: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Single step of the inference model.

        Returns (h_t, q_mu, q_logvar)
        """
        h_t = self.rnn(x_t, h_prev)
        q_mu = self.q_mu(h_t)
        q_logvar = self.q_logvar(h_t)
        return h_t, q_mu, q_logvar


class NSVMForecaster(BaseForecaster):
    """Neural Stochastic Volatility Model for Monte Carlo path generation."""

    def __init__(self, z_dim: int = 16, hidden_dim: int = 64) -> None:
        self.z_dim = z_dim
        self.hidden_dim = hidden_dim
        self.gen = GenerativeRNN(z_dim=z_dim, hidden_dim=hidden_dim)
        self.inf = InferenceRNN(z_dim=z_dim, hidden_dim=hidden_dim)
        self.last_price: float = 0.0
        self.last_h_gen: torch.Tensor | None = None
        self.last_h_inf: torch.Tensor | None = None
        self.last_z: torch.Tensor | None = None
        self._fitted = False

    def fit(
        self,
        prices: np.ndarray,
        epochs: int = 200,
        lr: float = 1e-3,
        batch_size: int = 32,
        **kwargs,
    ) -> None:
        """Train the NSVM on historical log returns."""
        prices = np.asarray(prices, dtype=np.float64)
        log_returns = np.diff(np.log(prices)).astype(np.float32)

        # Normalize returns for training stability
        self._return_mean = float(np.mean(log_returns))
        self._return_std = float(np.std(log_returns)) + 1e-8
        normed = (log_returns - self._return_mean) / self._return_std

        seq_len = min(96, len(normed) - 1)
        if seq_len < 10:
            logger.warning("Not enough data for NSVM training")
            self.last_price = float(prices[-1])
            return

        # Build sequences
        sequences = []
        for i in range(len(normed) - seq_len):
            sequences.append(normed[i : i + seq_len])
        X = torch.tensor(np.array(sequences), dtype=torch.float32).unsqueeze(-1)

        # Training
        params = list(self.gen.parameters()) + list(self.inf.parameters())
        optimizer = torch.optim.Adam(params, lr=lr)

        self.gen.train()
        self.inf.train()

        for epoch in range(epochs):
            # Random batch
            idx = np.random.choice(len(X), size=min(batch_size, len(X)), replace=False)
            batch = X[idx]  # (B, T, 1)
            B, T, _ = batch.shape

            # Initialize hidden states
            h_gen = torch.zeros(B, self.hidden_dim)
            h_inf = torch.zeros(B, self.hidden_dim)
            z = torch.zeros(B, self.z_dim)

            total_elbo = 0.0

            for t in range(T):
                x_t = batch[:, t, :]  # (B, 1)

                # Inference step
                h_inf, q_mu, q_logvar = self.inf(x_t, h_inf)

                # Generative step (prior)
                x_prev = batch[:, t - 1, :] if t > 0 else torch.zeros_like(x_t)
                h_gen, p_mu, p_logvar, e_mu, e_logvar = self.gen(x_prev, z, h_gen)

                # Sample z from q
                std = torch.exp(0.5 * q_logvar)
                eps = torch.randn_like(std)
                z = q_mu + std * eps

                # ELBO components
                # Reconstruction: log p(x_t | z_t, h_t)
                recon = -0.5 * (
                    e_logvar + (x_t - e_mu) ** 2 / (torch.exp(e_logvar) + 1e-8)
                )
                # KL: KL(q(z|x) || p(z|h))
                kl = 0.5 * (
                    p_logvar - q_logvar
                    + (torch.exp(q_logvar) + (q_mu - p_mu) ** 2) / (torch.exp(p_logvar) + 1e-8)
                    - 1
                )

                total_elbo += (recon.sum() - kl.sum()) / B

            loss = -total_elbo / T
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(params, 5.0)
            optimizer.step()

            if (epoch + 1) % 50 == 0:
                logger.debug("Epoch %d/%d ELBO=%.4f", epoch + 1, epochs, -loss.item())

        # Encode the last sequence for generation warm-start
        self.gen.eval()
        self.inf.eval()
        with torch.no_grad():
            last_seq = torch.tensor(normed[-seq_len:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)
            h_gen = torch.zeros(1, self.hidden_dim)
            h_inf = torch.zeros(1, self.hidden_dim)
            z = torch.zeros(1, self.z_dim)

            for t in range(seq_len):
                x_t = last_seq[:, t, :]
                h_inf, q_mu, q_logvar = self.inf(x_t, h_inf)
                x_prev = last_seq[:, t - 1, :] if t > 0 else torch.zeros_like(x_t)
                h_gen, _, _, _, _ = self.gen(x_prev, z, h_gen)
                z = q_mu  # Use posterior mean

            self.last_h_gen = h_gen
            self.last_h_inf = h_inf
            self.last_z = z

        self.last_price = float(prices[-1])
        self._fitted = True

    def generate_paths(
        self,
        asset: str,
        num_paths: int,
        num_steps: int,
        s0: float | None = None,
    ) -> np.ndarray:
        if not self._fitted:
            raise RuntimeError("Model not fitted")

        s0 = s0 or self.last_price

        with torch.no_grad():
            # Replicate initial states for all paths
            h_gen = self.last_h_gen.expand(num_paths, -1).contiguous()
            z = self.last_z.expand(num_paths, -1).contiguous()
            x_prev = torch.zeros(num_paths, 1)

            returns = np.zeros((num_paths, num_steps - 1), dtype=np.float64)

            for t in range(num_steps - 1):
                h_gen, p_mu, p_logvar, e_mu, e_logvar = self.gen(x_prev, z, h_gen)

                # Sample z from prior
                std_z = torch.exp(0.5 * p_logvar)
                z = p_mu + std_z * torch.randn_like(std_z)

                # Sample return from emission
                std_x = torch.exp(0.5 * e_logvar)
                x_t = e_mu + std_x * torch.randn_like(std_x)

                # Un-normalize
                raw_return = x_t.numpy()[:, 0] * self._return_std + self._return_mean
                returns[:, t] = raw_return
                x_prev = x_t

        # Convert to price paths
        cumulative = np.cumsum(returns, axis=1)
        paths = np.zeros((num_paths, num_steps), dtype=np.float64)
        paths[:, 0] = s0
        paths[:, 1:] = s0 * np.exp(np.clip(cumulative, -500, 500))

        return paths

    def params_dict(self) -> dict:
        return {
            "model": "NSVM",
            "z_dim": self.z_dim,
            "hidden_dim": self.hidden_dim,
            "last_price": self.last_price,
            "fitted": self._fitted,
        }

    def save(self, path: str | Path) -> None:
        torch.save({
            "gen": self.gen.state_dict(),
            "inf": self.inf.state_dict(),
            "return_mean": self._return_mean,
            "return_std": self._return_std,
        }, path)

    def load(self, path: str | Path) -> None:
        data = torch.load(path, weights_only=True)
        self.gen.load_state_dict(data["gen"])
        self.inf.load_state_dict(data["inf"])
        self._return_mean = data["return_mean"]
        self._return_std = data["return_std"]
        self.gen.eval()
        self.inf.eval()
